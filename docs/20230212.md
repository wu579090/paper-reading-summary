#### 2023.2.12 文献汇报

#### PCA,CCA及个体化

#### PCA

The dimension reduction technique should meet the following requirements:
(a) different components are independent; (b) there should be a index to measure the variation accounted by each component.
The simplest way to reduce the dimension is PCA(principal component analysis).
PCA is a linear transformation from a set of variables $\vec{x}_1, \cdots, \vec{x}_p$ to $\vec{y}_1, \cdots, \vec{y}_p$, where $\vec{x}_i$s may be correlated to each other, but $\vec{y}_i$s are uncorrelated to each other.
We use covariance to describe the correlation between two random variables.
We first have matrix of observations, where each row is a random variable and each column is an observation. Suppose we have two sets of variables and we observe them for $N$ times.
$$
	X = 
	\begin{bmatrix}
		w_1 & w_2 & \cdots & w_N\\
		h_1 & h_2 & \cdots & h_N
	\end{bmatrix}
	=
	\begin{bmatrix}
		\vec{\mathbf{X}}_1 & \cdots & \vec{\mathbf{X}}_N
	\end{bmatrix}
$$
 To prepare for PCA, we should let the observation matrix in mean-deviation form, that is to say to the average of each random variable should be 0. This could be done by subtracting the sample mean from each observations.

$$
 	\mathbf{M} = \frac{1}{N}\left( \vec{\mathbf{X}}_1 + \cdots + \vec{\mathbf{X}}_N \right)
$$

$$
	B = 
	\begin{bmatrix}
		\vec{\mathbf{X}}_1-\mathbf{M} & \cdots & \vec{\mathbf{X}}_N-\mathbf{M}
	\end{bmatrix}
$$

Sample covariance matrix $S$ is the $p\times p$ symmetric matrix, where $p$ is the number of variables:

$$
	S = \frac{1}{N-1}BB^T
$$
Diagonal entry $s_{jj}$ is the variance of $j$th random variable. If two random variables are uncorrelated, the covariance of them is $0$. The trance of $S$ is the total variance of the data.

We are in hope for a linear transformation from $X$ to $Y=PX$ whose sample covariance matrix is diagonal.

$$
	& S_X = \frac{1}{N-1}XX^T \\
	& S_Y = \frac{1}{N-1}(PX)(PX)^T = \frac{1}{N-1}(PX)(X^TP^T) = P\frac{(XX^T)}{N-1}P^T = PS_XP^T
$$
In the meanwhile, we don't want to change the total variance, thus the trace of two covariance should be the same: $\mathrm{tr}(S_X) = \mathrm{tr}(S_Y)$, thus $S_X$ and $S_Y$ are similar to each other. Given:

$$
	S_X = P^TS_YP
	\label{eq:pca1}
$$
where $S_Y$ is diagonal, then $P$ must be orthogonal matrix that meet $P^T=P^{-1}$.
Reconsider \autoref{eq:pca1}, where $S_X$ is factorized into the product of three matrix, it is exactly the orthogonal diagonalization of $S_X$. Hence $P$ is the orthogonal matrix whose rows are the corresponding unit eigenvectors of the $S_X$.

We ordered the eigenvalues of $S_X$: $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_p$. $\lambda_i/\mathrm{tr}(S_x)$ is the $i$th component's variance accounting for the whole variance. Thus we preserve the component of high eigenvalue and discard the component of low eigenvalue.

#### CCA与个体化

CCA将两组随机变量 $X_1, \cdots, X_n$ ，$Y_1, \cdots, Y_m$ 连接做回归分析的方法，即寻找两组随机变量的线性组合 $U=c_1X_1+\cdots+c_nX_n$ 与 $V=d_1Y_1+\cdots+d_mY_m$ ，使得 $U$ 与 $V$ 的相关最大。本文使了 CCA 求得功能连接与个体行为或特质上的多对多的联系。

个体化在 wide data 流行后（让一个被试的数据也有了足够的信噪比）使人们意识到，一个个体的功能组织不仅与其他人的功能组织差异较大，并且与总体的平均功能组织也相差较大。因此在个体水平上定位功能组织，是非常重要的课题。



**参考文献**

>Smith, S. M., Nichols, T. E., Vidaurre, D., Winkler, A. M., Behrens, T. E. J., Glasser, M. F., Ugurbil, K., Barch, D. M., Van Essen, D. C., & Miller, K. L. (2015). A positive-negative mode of population covariation links brain connectivity, demographics and behavior. *Nature Neuroscience*, *18*(11), 1565–1567. https://doi.org/10.1038/nn.4125

